import autogen;

def reflection_message(recipient, messages, sender, config):
    print(f"Last chat message: {recipient.chat_messages_for_summary(sender)[-1]['content']}")
    print(f"Second message: {messages[0]['content']}");
    return f"Make sure the description of the code provided matches the actual contents of the code and provide feedback. Description: {recipient.chat_messages_for_summary(sender)[-1]['content']} \n Code: {messages[0]['content']}";

def reflection_message_2(recipient: autogen.ConversableAgent, messages, sender, config):
    print(f"All messages so far: {messages}, recipient messages: {recipient.chat_messages_for_summary(sender)}, sender messages: {sender.chat_messages_for_summary(recipient)}");
    return f"Make sure the code provided matches the description. Description: {recipient.chat_messages_for_summary(sender)[-1]['content']} \n Code: {messages[0]['content']}";

def get_code_output(sender, recipient, summary_args):
    print(f"summary_args {summary_args}")
    print(f"Last message for summary#1 {sender.chat_messages_for_summary(recipient)[-2]['content']}")
    return sender.chat_messages_for_summary(recipient)[-2]['content'];

def get_last_msg(sender, recipient, summary_args):
    return sender.chat_messages_for_summary(recipient)[-1]['content'];
def main():
    config_list = autogen.config_list_from_json(env_or_file='.config/ollama.json');
    llm_config = {
        "cache_seed": None,  # change the cache_seed for different trials
        "temperature": 0,
        "config_list": config_list,
        "timeout": 600,  # in seconds
        "stream": True
    }
    coder = autogen.AssistantAgent(
        name="Coder", 
        llm_config = llm_config, 
        system_message="""You are a professional software developer with 50 years of experience developing python scripts. You very carefully read requests to you and provide succinct responses. Remember that you are on a windows machine and that any code you provide must take that into account. Once you have received a response to your code that is not an error, reply with 'TERMINATE'""",
        max_consecutive_auto_reply=2,
        description="Provides python code for user_proxy to run."
    );
    describer = autogen.AssistantAgent(
        name="CPP_Describer", 
        llm_config = llm_config, 
        system_message="""You are a novice business analyst with over 20 years of development experience. You are very proficient at reading code and translating it into a series of human readable step-by-step instructions the code executes. You always explain any assumptions you make and thoroughly evaluate possibilites if those assumptions are false. You must read any feedback provided by CPP_Reviewer and after taking it into account produce a full description of the code with the corrections made. Return the complete description followed by 'TERMINATE' once CPP_Reviewer has mentioned the description is good enough.""",
        max_consecutive_auto_reply=4,
        description="Reads CPP code printed out by user_proxy and prints out a human readable step-by-step breakdown of the code",
    );
    reviewer = autogen.AssistantAgent(name="CPP_Reviewer", llm_config = llm_config, system_message="""You are an experienced tech-lead who has been at the forefront of technology from the birth of the modern PC until now. You are very proficient at reading both code and code translated into instructions. Your primary task is to read output from CPP_Describer as well as the contents of a C++ file it supposedly describes, and providing pointed and useful feedback that CPP_Reader can use to improve the description. If the output from CPP_Describer seems good enough, reply with 'TERMINATE'.""",
    description="Reviews output from CPP_Describer",
    max_consecutive_auto_reply=2);
    user_proxy = autogen.UserProxyAgent(
        name="user_proxy", 
        code_execution_config={
            "work_dir": "code_output",
            "use_docker": False
        }, 
        is_termination_msg=lambda msg: msg.get("content") is not None and "TERMINATE" in msg["content"],
        default_auto_reply="TERMINATE",
        description="Runs code only after the code has been generated by Coder. This agent will never be selected first",
        human_input_mode="NEVER"
    );
    python_coder = autogen.AssistantAgent(name="python_Coder", llm_config = llm_config, system_message="""You are an expert python coder who is known for carefully thinking through alternatives before writing anything and heavily commenting code. Your goal is to take output generated by CPP_Describer and produce a complete python code file that matches those requirements. Make sure to put # filename: <filename> inside the code block as the first line. Once you have received and implemented feedback from python_Reviewer, reply with "TTERMINATE." """,max_consecutive_auto_reply=3, description="Reads requirements and translates them into python code");
    python_reviewer = autogen.AssistantAgent(name="python_Reviewer", llm_config = llm_config, system_message="""You are an experienced tech-lead who oversees a large team of inexperienced developers. You evaluate the work of python_Coder against the requirements python_Coder was originally given, and provide pointed and actionable feedback that python_Coder can use to improve output. If you are happy with the output from python_Coder, reply only with "TERMINATE.""", max_consecutive_auto_reply=3, description="Reviews output from python Coder");

    """
    group_chat = autogen.GroupChat(
        agents=[user_proxy, coder, describer, reviewer], messages=[], max_round=12,
        select_speaker_prompt_template="Read the above conversation carefully. Then, select the role from {agentlist} that is best able to respond to the above message. Respond only with the agent name and DO NOT provide an explanation")
    
    manager = autogen.GroupChatManager(groupchat=group_chat, llm_config=llm_config)

    """
    user_proxy.register_nested_chats(
        [
            {
                "recipient": reviewer, "message": reflection_message, "summary_method": "last_msg", "max_turns": 1
            }
        ],
        trigger=describer
    )
    user_proxy.register_nested_chats(
        [
            {
                "recipient": python_reviewer, "message": reflection_message_2, "summary_method": "last_msg", "max_turns": 1
            }
        ],
        trigger=python_coder
    )
    user_proxy.initiate_chats([
            {
                "recipient": coder,
                "message": "Provide some code to read the contents of 'C:\\\\Users\\\\<hidden>\\\\Documents\\\\GenAI\\\\autogen\\\\files\\\\testFile.cpp'",
                "clear_history": "True",
                "silent": False,
                "summary_method": get_code_output,
                "max_turns": 2
            },
            {
                "recipient": describer,
                "message": "Describe the code from the previous message",
                "clear_history": True,
                "silent": False,
                "summary_method": get_last_msg,
                "max_turns": 4
            },
            {
                "recipient": python_coder,
                "message": "write a complete python code file that matches the requirements from the above message",
                "clear_history": "True",
                "silent": False,
                "summary_method": get_last_msg,
                "max_turns": 2
            }        
        ]);
    

if __name__ == "__main__":
    main();